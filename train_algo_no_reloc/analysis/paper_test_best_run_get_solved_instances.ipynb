{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K\n",
    "import convert_event_list as convert\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import time\n",
    "import math\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import colorsys\n",
    "import numpy as np\n",
    "    \n",
    "from data_retrieval_3ksol import INSTANCEProvider\n",
    "from kbh_yard_b2b import KBH_Env                               #This is the environment of the shunting yard\n",
    "from dqn_kbh_colfax_b2b_instances_test_agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = range(5000,8020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1, 33, 32)         16928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 33, 64)         8256      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2112)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               540928    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 568,425\n",
      "Trainable params: 568,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 1, 33, 32)         16928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 33, 64)         8256      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2112)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               540928    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 568,425\n",
      "Trainable params: 568,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "target_model_updated\n",
      "model loaded\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "Model:  1521138212\n",
      "   original_length  solved  tried  percentage\n",
      "0               23     644    755    0.852980\n",
      "1               26     626    755    0.829139\n",
      "2               27     645    755    0.854305\n",
      "3               29     654    755    0.866225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1360e91d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_nr in ['1521138212']:\n",
    "    #which model to load.\n",
    "    test_case = model_nr\n",
    "    \n",
    "    #LOAD THE INSTANCE PROVIDER\n",
    "    ig = INSTANCEProvider()\n",
    "    \n",
    "    # Create environment KBH\n",
    "    yrd = KBH_Env()\n",
    "\n",
    "    # Create the DQNAgent with the CNN approximation of the Q-function and its experience replay and training functions.\n",
    "    # load the trained model.\n",
    "    agent = DQNAgent(yrd, True, test_case)\n",
    "\n",
    "    # set epsilon to 0 to act just greedy\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    #new_cmap = rand_cmap(200, type='soft', first_color_black=True, last_color_black=False, verbose=True)\n",
    "    \n",
    "    visualization = False\n",
    "    \n",
    "    n = len(instances)\n",
    "    \n",
    "    # result vectors\n",
    "    original_lengths = []\n",
    "    terminated_at_step = []\n",
    "    print_count = 0\n",
    "    \n",
    "    \n",
    "    # train types different tracks? \n",
    "    type_step_track = []\n",
    "    \n",
    "       \n",
    "    for instance in instances:\n",
    "        if print_count % 100 == 0:\n",
    "            print(print_count)\n",
    "        print_count = print_count + 1\n",
    "        #Initialize problem\n",
    "        event_list = ig.get_instance(instance)\n",
    "        \n",
    "        steps = len(event_list)\n",
    "        if len(event_list) < 60:        \n",
    "            t = 0\n",
    "    \n",
    "            score = 0  # Init score variable for this particular episode.\n",
    "    \n",
    "            state = yrd.reset(event_list)  # Get first observation based on the first train arrival.\n",
    "            history = np.reshape(state, (\n",
    "                1, yrd.shape[0], yrd.shape[1], yrd.shape[2]))  # reshape state into tensor, which we call history.\n",
    "            \n",
    "            if visualization == True:\n",
    "                plt.imshow(np.float32(history[0][0]), cmap=new_cmap, interpolation='nearest')\n",
    "                plt.show()\n",
    "    \n",
    "            while t <= steps:\n",
    "                action = agent.get_action(history)  # RL choose action based on observation\n",
    "                \n",
    "                if visualization == True:\n",
    "                    print(agent.model.predict(history))\n",
    "                    print(action+1)\n",
    "                # RL take action and get next observation and reward\n",
    "                # note the +1 at action\n",
    "                event_list_temp = event_list.reset_index(drop=True).copy()\n",
    "                if event_list_temp.event_type[0]=='arrival':\n",
    "                    train_type = event_list_temp.composition[0]\n",
    "                    type_step_track.append({'type': train_type, 'action': action+1, 'step':t, 'instance_id': instance})\n",
    "                    \n",
    "                raw_observation_, reward, done = yrd.step(action + 1, event_list)\n",
    "    \n",
    "                state_ = np.reshape(raw_observation_, (1, yrd.shape[0], yrd.shape[1], yrd.shape[2]))  # reshape to tensor.\n",
    "                history_ = state_  # this is now the next observation, call it history_\n",
    "    \n",
    "                score += reward  # log direct reward of action\n",
    "                \n",
    "                \n",
    "                \n",
    "                if visualization == True: \n",
    "                    #show action\n",
    "                    plt.imshow(np.float32(history_[0][0]), cmap=new_cmap, interpolation='nearest')\n",
    "                    plt.show()\n",
    "                    time.sleep(0.05)\n",
    "                    if reward == -1:\n",
    "                        time.sleep(1)\n",
    "                    print(reward)\n",
    "                    \n",
    "                if done:  # based on what the environment returns.\n",
    "                    #print('ended at step' , t+1)\n",
    "                    #print('original length', steps)\n",
    "                    original_lengths.append(steps)\n",
    "                    terminated_at_step.append(t+1)\n",
    "                    break;\n",
    "    \n",
    "    \n",
    "                history = history_  # next state now becomes the current state.\n",
    "                t += 1  # next step in this episode\n",
    "        else: \n",
    "            original_lengths.append(steps)\n",
    "            terminated_at_step.append(-1)\n",
    "    \n",
    "    #compute scores\n",
    "    df_type_step_track = pd.DataFrame.from_records(type_step_track)\n",
    "    df_type_step_track['strtype'] = df_type_step_track.apply(lambda row: str(row.type), axis = 1)\n",
    "    df_type_step_track.strtype = df_type_step_track.strtype.astype('category')\n",
    "    \n",
    "    analysis_runs = pd.DataFrame(\n",
    "    {'instance_id': instances,\n",
    "     'original_length': original_lengths,\n",
    "     'terminated_at_step': terminated_at_step\n",
    "    })\n",
    " \n",
    "    analysis_runs['solved'] = analysis_runs.apply(lambda row: 1 if row.original_length == row.terminated_at_step else 0, axis =1 )\n",
    "    analysis_runs['tried'] = analysis_runs.apply(lambda row: 1 if row.terminated_at_step != -1 else 0, axis =1)\n",
    "    analysis_runs['percentage'] = analysis_runs.apply(lambda row: row.solved/755, axis=1)\n",
    "    \n",
    "    analysis_runs.to_csv('best_model_solved_instances.csv')\n",
    "    print('Model: ', model_nr)\n",
    "    summary = analysis_runs.groupby('original_length', as_index=False)[['solved', 'tried', 'percentage']].sum()\n",
    "    print(summary)\n",
    "    \n",
    "    #print hist\n",
    "    %matplotlib inline  \n",
    "    #%%\n",
    "    # analyse the parking actions per step and train type\n",
    "    df_type_step_track = pd.DataFrame.from_records(type_step_track)\n",
    "    bins = [1,2,3,4,5,6,7,8,9,10]\n",
    "    plt.hist(df_type_step_track.action, bins, align='left')\n",
    "    \n",
    "    #prepare for save\n",
    "    df_type_step_track['strtype'] = df_type_step_track.apply(lambda row: str(row.type), axis = 1)\n",
    "    df_type_step_track.strtype = df_type_step_track.strtype.astype('category')\n",
    "    filename = 'data_'+model_nr+'_paper.csv'\n",
    "    df_type_step_track.to_csv(filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
